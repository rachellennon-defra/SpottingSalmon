{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89e62505-546b-4027-86d9-ba5fdf194a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Generate labelled resistivity data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "310ea0ec-4a43-4f1f-848e-17fbb6938515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loop through the raw daily .csv files in the dbfs and align with the master event file (too big to combine and do in one). Save the labelled data to a new folder called labelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e5429b-0da1-4034-9cb0-3a9a235406ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load and prepare events file with all of the known fish events\n",
    "events_path = \"/dbfs/FileStore/rachlenn/Thr 20 process/test_KMThu16_2021_07.csv\"\n",
    "events = pd.read_csv(events_path, header=0)\n",
    "\n",
    "# Ensure datetime is timezone-aware\n",
    "events['Time'] = pd.to_datetime(events['Time'], utc=True)\n",
    "\n",
    "# Create +/- event windows 2 seconds before and 8 seconds after as per AF parameters - we might want to truncate this?\n",
    "events['start_time'] = events['Time'] - pd.Timedelta(seconds=2)\n",
    "events['end_time']   = events['Time'] + pd.Timedelta(seconds=8)\n",
    "\n",
    "\n",
    "# Function to label fish presence from the timestamps in the raw df \n",
    "def label_fish_presence(df, events_df):\n",
    "    df['Time'] = pd.to_datetime(df['Time'], utc=True)\n",
    "    df['fish_present'] = 0\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        mask = (df['Time'] >= event['start_time']) & (df['Time'] <= event['end_time'])\n",
    "        df.loc[mask, 'fish_present'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Loop through all raw daily datasets\n",
    "input_pattern = \"/dbfs/FileStore/rachlenn/Thr 20 process/*Z\"  # dailies \n",
    "output_folder = \"/dbfs/FileStore/rachlenn/labeled\" # where to put the labelled dailies\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run labelling function on all of the daily datasets\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Load and prep daily data\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    df.columns = [\"timestamp\", \"upstream\", \"downstream\"]\n",
    "\n",
    "    # Convert ms timestamp to datetime making sure it is timezone aware too\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "    df = df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "    # Add the differential conductance to the daily df \n",
    "    df[\"differential_conductance\"] = (df[\"downstream\"] - df[\"upstream\"]) / 2\n",
    "\n",
    "    # Label fish events using the function \n",
    "    labeled_df = label_fish_presence(df, events)\n",
    "\n",
    "    # Save to new CSVs in the output folder\n",
    "    filename = os.path.basename(file_path).replace(\".csv\", \"_labeled.csv\")\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    labeled_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"All datasets labelled and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f29ca12c-cbb8-4d1e-b82e-95e13ee23d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Extract Windows**\n",
    "\n",
    "The UNet model should be fed windows of the event, so need to generate these before balancing the date stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012c86e8-30b2-4707-bbc9-8450ab80f08b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "input_folder = \"/dbfs/FileStore/rachlenn/labeled/\"\n",
    "window_duration_seconds = 10\n",
    "\n",
    "sampling_rates = []\n",
    "\n",
    "# First pass: compute all sampling rates\n",
    "for file_path in glob.glob(input_folder + \"*Z\"):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Time\"], utc=True)\n",
    "    num_samples = len(df)\n",
    "    time_delta = (df[\"Time\"].iloc[-1] - df[\"Time\"].iloc[0]).total_seconds()\n",
    "    sps = num_samples / time_delta\n",
    "    sampling_rates.append(sps)\n",
    "\n",
    "median_sps = np.median(sampling_rates)\n",
    "print(f\"Median sampling rate: {median_sps} Hz\")\n",
    "\n",
    "fixed_window_size = int(median_sps * window_duration_seconds)\n",
    "print(f\"Fixed window size: {fixed_window_size} samples per window\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38842f54-efa8-4668-9a6a-cf1a63b3d141",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "input_folder = \"/dbfs/FileStore/rachlenn/labeled/\"\n",
    "output_folder = \"/dbfs/FileStore/rachlenn/windows_balanced/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "feature_cols = [\"upstream\", \"downstream\", \"differential_conductance\"]\n",
    "label_col = [\"fish_present\"]\n",
    "\n",
    "def create_windows_fixed(df, window_size):\n",
    "    df = df.sort_values(\"Time\").reset_index(drop=True)\n",
    "    data = df[feature_cols].values\n",
    "    labels = df[label_col].values.flatten()\n",
    "    \n",
    "    X, y = [], []\n",
    "    step = window_size  # no overlap\n",
    "    \n",
    "    for start in range(0, len(df) - window_size + 1, step):\n",
    "        end = start + window_size\n",
    "        window_features = data[start:end]\n",
    "        window_label = 1 if np.any(labels[start:end] == 1) else 0\n",
    "        X.append(window_features)\n",
    "        y.append(window_label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def balance_windows(X, y):\n",
    "    fish_indices = np.where(y == 1)[0]\n",
    "    nonfish_indices = np.where(y == 0)[0]\n",
    "    num_fish = len(fish_indices)\n",
    "    if num_fish == 0:\n",
    "        return X, y  # no fish windows, return as-is\n",
    "    \n",
    "    sampled_nonfish_indices = np.random.choice(nonfish_indices, size=num_fish, replace=False)\n",
    "    balanced_indices = np.concatenate([fish_indices, sampled_nonfish_indices])\n",
    "    np.random.shuffle(balanced_indices)\n",
    "    \n",
    "    return X[balanced_indices], y[balanced_indices]\n",
    "\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "for file_path in glob.glob(os.path.join(input_folder, \"*Z\")):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"Time\"], utc=True)\n",
    "    \n",
    "    X, y = create_windows_fixed(df)\n",
    "    X_balanced, y_balanced = balance_windows(X, y)\n",
    "    \n",
    "    # Save balanced windows per file if needed, or just append\n",
    "    np.save(os.path.join(output_folder, os.path.basename(file_path).replace(\"Z\", \"_X_balanced.npy\")), X_balanced)\n",
    "    np.save(os.path.join(output_folder, os.path.basename(file_path).replace(\"Z\", \"_y_balanced.npy\")), y_balanced)\n",
    "    \n",
    "    all_X.append(X_balanced)\n",
    "    all_y.append(y_balanced)\n",
    "\n",
    "# Combine all balanced windows\n",
    "all_X = np.concatenate(all_X)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "print(f\"Combined balanced windows shape: {all_X.shape}, labels shape: {all_y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "319cebe4-5fcd-43ea-8d4d-651a9a085670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Balance training dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1dd610e-25e2-4260-b0ff-a654f9f446bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The noise (0/no fish) to signal (1/fish) ratio is out of whack. We need to balance it so the ratio is more 1:1 and can be used as a training dataset. Otherwise it could label everything as 0 and we would still get 99% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1083af38-4f78-4c98-9e8b-15d664b41b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get the paths to the right folders\n",
    "input_folder = \"/dbfs/FileStore/rachlenn/labeled/\" # Where the labelled daily datasets are\n",
    "output_folder = \"/dbfs/FileStore/rachlenn/balanced/\" # Where to put the balanced df \n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all CSV files in input folder that end in Z\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"*Z\"))\n",
    "\n",
    "# Initiate empty df\n",
    "balanced_dfs = []\n",
    "\n",
    "# Run through all of the labelled daily df and balance the noise to signal\n",
    "for file in csv_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Split fish vs no-fish\n",
    "    fish_df = df[df['fish_present'] == 1]\n",
    "    no_fish_df = df[df['fish_present'] == 0]\n",
    "    \n",
    "    # Match counts so equalised\n",
    "    no_fish_sample = no_fish_df.sample(len(fish_df), random_state=42)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat([fish_df, no_fish_sample]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Save balanced CSV\n",
    "    filename = os.path.basename(file).replace(\".csv\", \"_balanced.csv\")\n",
    "    balanced_path = os.path.join(output_folder, filename)\n",
    "    balanced_df.to_csv(balanced_path, index=False)\n",
    "    \n",
    "    balanced_dfs.append(balanced_df)\n",
    "\n",
    "# Merge into one big dataset\n",
    "merged_df = pd.concat(balanced_dfs).sample(frac=1, random_state=42)\n",
    "merged_df.to_csv(os.path.join(output_folder, \"all_balanced.csv\"), index=False)\n",
    "\n",
    "print(\" All balanced CSVs and master df saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LabelledDf",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
