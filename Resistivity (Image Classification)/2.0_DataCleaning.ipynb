{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567ad32d-3ca4-4011-84d9-12099a3957e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Generate labelled resistivity data** \n",
    "\n",
    "Loop through the raw daily .csv files in the dbfs and align with the master event file so we have fish events aligned to the resistivity data. Save the labelled data to a new folder called labelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b7082a-73f5-437f-984a-712520ffc6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load and prepare events file with all of the known fish events\n",
    "events_path = \"/dbfs/FileStore/rachlenn/Thr 20 process/test_KMThu16_2021_07_eventonly.csv\"\n",
    "events = pd.read_csv(events_path, header=0)\n",
    "\n",
    "# Ensure datetime is timezone-aware\n",
    "events['Time'] = pd.to_datetime(events['Time'], utc=True)\n",
    "\n",
    "# Create +/- event windows 2 seconds before and 2 seconds after as per AF parameters - we might want to truncate this?\n",
    "events['start_time'] = events['Time'] - pd.Timedelta(seconds=2.5)\n",
    "events['end_time']   = events['Time'] + pd.Timedelta(seconds=2.5)\n",
    "\n",
    "\n",
    "# Function to label fish presence from the timestamps in the raw df \n",
    "def label_fish_presence(df, events_df):\n",
    "    df['Time'] = pd.to_datetime(df['Time'], utc=True)\n",
    "    df['fish_present'] = 0\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        mask = (df['Time'] >= event['start_time']) & (df['Time'] <= event['end_time'])\n",
    "        df.loc[mask, 'fish_present'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Loop through all raw daily datasets\n",
    "input_pattern = \"/dbfs/FileStore/rachlenn/DuplicateFree/*_no_duplicate\"  \n",
    "output_folder = \"/dbfs/FileStore/rachlenn/labeled\" # where to put the labelled dailies\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run labelling function on all of the daily datasets\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Load and prep daily data\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    df.columns = [\"timestamp\", \"upstream\", \"downstream\"]\n",
    "\n",
    "    # Convert ms timestamp to datetime making sure it is timezone aware too\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "    df = df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "    # Add the differential conductance to the daily df \n",
    "    df[\"differential_conductance\"] = (df[\"downstream\"] - df[\"upstream\"]) / 2\n",
    "\n",
    "    # Label fish events using the function \n",
    "    labeled_df = label_fish_presence(df, events)\n",
    "\n",
    "    # Save to new CSVs in the output folder\n",
    "    filename = os.path.basename(file_path).replace(\"_no_duplicate\", \"_labelled\")\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    labeled_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"All datasets labelled and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad147773-cf3e-4095-b1c2-170fa6109726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Extract Windows**\n",
    "\n",
    "The ML models should be fed windows of the event, so need to generate these before balancing the date stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4468217c-489a-4bee-ab48-24f8539b1fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "input_pattern = \"/dbfs/FileStore/rachlenn/labeled/*_labelled\"\n",
    "output_file = \"/dbfs/FileStore/rachlenn/windows_10s.csv\"\n",
    "\n",
    "window_size_seconds = 10\n",
    "all_windows = []\n",
    "\n",
    "for file in glob.glob(input_pattern):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Time'] = pd.to_datetime(df['Time'], utc=True)\n",
    "    df = df.sort_values('Time')\n",
    "\n",
    "    # Start at the earliest timestamp and move in 10-second steps\n",
    "    start_time = df['Time'].min()\n",
    "    end_time = df['Time'].max()\n",
    "\n",
    "    current_start = start_time\n",
    "    while current_start < end_time:\n",
    "        current_end = current_start + pd.Timedelta(seconds=window_size_seconds)\n",
    "\n",
    "        window_df = df[(df['Time'] >= current_start) & (df['Time'] < current_end)]\n",
    "\n",
    "        if not window_df.empty:\n",
    "            fish_label = 1 if window_df['fish_present'].any() else 0\n",
    "            all_windows.append({\n",
    "                \"start_time\": current_start,\n",
    "                \"end_time\": current_end,\n",
    "                \"upstream_values\": window_df['upstream'].tolist(),\n",
    "                \"downstream_values\": window_df['downstream'].tolist(),\n",
    "                \"diff_values\": window_df['differential_conductance'].tolist(),\n",
    "                \"label\": fish_label\n",
    "            })\n",
    "\n",
    "        current_start = current_end\n",
    "\n",
    "# Combine into one DataFrame\n",
    "windows_df = pd.DataFrame(all_windows)\n",
    "\n",
    "# Save for training\n",
    "windows_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved {len(windows_df)} windows to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "682dd378-4387-48f4-bb8a-ceb20e17f90c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Balance training dataset**\n",
    "\n",
    "The noise (0/no fish) to signal (1/fish) ratio is out of whack. We need to balance it so the ratio is more 1:1 and can be used as a training dataset. Otherwise it could label everything as 0 and we would still get 99% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e97c44f9-bf6e-4784-92b3-d6d17d7c50d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get the paths to the right folders\n",
    "input_folder = \"/dbfs/FileStore/rachlenn/labeled/\" # Where the labelled daily datasets are\n",
    "output_folder = \"/dbfs/FileStore/rachlenn/balanced/\" # Where to put the balanced df \n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all CSV files in input folder that end in Z\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"*Z\"))\n",
    "\n",
    "# Initiate empty df\n",
    "balanced_dfs = []\n",
    "\n",
    "# Run through all of the labelled daily df and balance the noise to signal\n",
    "for file in csv_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Split fish vs no-fish\n",
    "    fish_df = df[df['fish_present'] == 1]\n",
    "    no_fish_df = df[df['fish_present'] == 0]\n",
    "    \n",
    "    # Match counts so equalised\n",
    "    no_fish_sample = no_fish_df.sample(len(fish_df), random_state=42)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat([fish_df, no_fish_sample]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Save balanced CSV\n",
    "    filename = os.path.basename(file).replace(\".csv\", \"_balanced.csv\")\n",
    "    balanced_path = os.path.join(output_folder, filename)\n",
    "    balanced_df.to_csv(balanced_path, index=False)\n",
    "    \n",
    "    balanced_dfs.append(balanced_df)\n",
    "\n",
    "# Merge into one big dataset\n",
    "merged_df = pd.concat(balanced_dfs).sample(frac=1, random_state=42)\n",
    "merged_df.to_csv(os.path.join(output_folder, \"all_balanced.csv\"), index=False)\n",
    "\n",
    "print(\" All balanced CSVs and master df saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.0_DataCleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
