{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "152c8980-0cc8-4b92-a0e3-52520bfd9110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Random Forest Resistivity Approach**\n",
    "\n",
    "Apply sliding window to random forest classification model to predict presence/absence of fish events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f37f2a-b89c-4ae1-b504-b2f0759e53ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inspect dataframe\n",
    "df = pd.read_csv(\"/dbfs/FileStore/rachlenn/labeled/test_KMThu16_2021_07_15_15_36_24Z\")\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3555f40-cdc2-47f8-9a34-53fd0f7bce35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Read all CSVs into a single Spark DataFrame\n",
    "spark_df = spark.read.csv(\"/FileStore/rachlenn/labeled/*Z\", header=True, inferSchema=True)\n",
    "\n",
    "# Sort by time\n",
    "spark_df = spark_df.sort(\"Time\")\n",
    "\n",
    "# Convert to pandas\n",
    "pdf = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33e20c31-84dd-47d3-9ab5-e90ca6260377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Make a sliding window of ~10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a95dd3-9691-4178-893d-35620a02d655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Total number of samples\n",
    "num_samples = len(pdf)\n",
    "\n",
    "# Time difference in seconds\n",
    "full_pdf['Time'] = pd.to_datetime(full_pdf['Time']); time_delta = (full_pdf[\"Time\"].iloc[-1] - full_pdf[\"Time\"].iloc[0]).total_seconds()\n",
    "\n",
    "# Samples per second\n",
    "sps = num_samples / time_delta\n",
    "\n",
    "print(f\"Samples per second: {sps:.2f}\")\n",
    "\n",
    "# Make a window frame of 10 seconds (may need to change this later on but discuss with AF)\n",
    "window_duration_seconds = 10\n",
    "\n",
    "window_size = int(sps * window_duration_seconds)\n",
    "\n",
    "print(f\"Rows per 10 sec window: {window_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6546340a-177a-4234-b6a3-36dbf8a14b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Engineer additional features for the model to \"learn\" from as this is traditional ML and not a NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83864d9-689b-4627-a1e3-906bdfa62685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Feature Engineering\n",
    "\n",
    "# Step size\n",
    "step_size = int(window_size/2)  #  50% overlap of frame - can change?\n",
    "\n",
    "# Initiate empty df\n",
    "feature_rows = []\n",
    "\n",
    "# Extract features over windows\n",
    "for start in range(0, len(pdf) - window_size + 1, step_size):\n",
    "    window = pdf.iloc[start:start + window_size]\n",
    "\n",
    "    features = {}\n",
    "    features[\"window_start_time\"] = window[\"Time\"].iloc[0]\n",
    "    features[\"window_end_time\"] = window[\"Time\"].iloc[-1]\n",
    "\n",
    "    for col in [\"upstream\", \"downstream\", \"differential_conductance\"]:\n",
    "        features[f\"{col}_mean\"] = window[col].mean()\n",
    "        features[f\"{col}_std\"] = window[col].std()\n",
    "        features[f\"{col}_min\"] = window[col].min()\n",
    "        features[f\"{col}_max\"] = window[col].max()\n",
    "        features[f\"{col}_energy\"] = (window[col]**2).sum()\n",
    "\n",
    "    feature_rows.append(features)\n",
    "\n",
    "# Convert to new df\n",
    "features_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "# Check\n",
    "display(features_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f6a794-4cb2-4208-927f-921ce1a75a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2439763f-1bd3-481e-b29d-57874ee14b1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Add fish events - pray to God this exists already \n",
    "fish_events = [\n",
    "    (\"2021-07-16T22:20:00Z\", \"2021-07-16T22:22:03Z\"),\n",
    "    (\"2021-07-17T05:53:21Z\", \"2021-07-17T05:53:24Z\"),\n",
    "    (\"2021-07-17T07:25:29Z\", \"2021-07-17T07:25:22Z\"),\n",
    "    (\"2021-07-17T14:10:23Z\", \"2021-07-17T14:10:26Z\"),\n",
    "    (\"2021-07-17T14:48:28Z\", \"2021-07-17T14:48:31Z\"),\n",
    "    (\"2021-07-18T06:22:04Z\", \"2021-07-18T06:22:07Z\"),\n",
    "    (\"2021-07-18T18:52:47Z\", \"2021-07-18T18:53:00Z\"),\n",
    "    (\"2021-07-18T22:28:41Z\", \"2021-07-18T22:28:44Z\"),\n",
    "    (\"2021-07-18T22:28:45Z\", \"2021-07-18T22:28:48Z\"),\n",
    "    (\"2021-07-19T06:31:20Z\", \"2021-07-19T06:31:23Z\"),\n",
    "    (\"2021-07-19T06:34:17Z\", \"2021-07-19T06:34:20Z\"),\n",
    "    (\"2021-07-19T06:57:29Z\", \"2021-07-19T06:57:32Z\"),\n",
    "    (\"2021-07-19T08:44:38Z\", \"2021-07-19T08:44:41Z\"),\n",
    "    (\"2021-07-19T14:14:20Z\", \"2021-07-18T14:14:23Z\"),\n",
    "    (\"2021-07-19T16:36:06Z\", \"2021-07-19T16:36:09Z\"),\n",
    "    (\"2021-07-19T16:41:30Z\", \"2021-07-19T16:41:33Z\"),\n",
    "    (\"2021-07-19T16:55:35Z\", \"2021-07-19T16:55:38Z\"),\n",
    "    (\"2021-07-19T17:05:45Z\", \"2021-07-19T17:05:48Z\"),\n",
    "    (\"2021-07-19T17:40:18Z\", \"2021-07-19T17:40:21Z\"),\n",
    "    (\"2021-07-19T19:08:54Z\", \"2021-07-19T19:08:57Z\"),\n",
    "    (\"2021-07-19T19:26:00Z\", \"2021-07-19T19:26:03Z\"),\n",
    "    (\"2021-07-20T07:08:02Z\", \"2021-07-20T07:08:05Z\"),\n",
    "    (\"2021-07-20T07:12:20Z\", \"2021-07-20T07:12:23Z\"),\n",
    "    (\"2021-07-20T07:14:19Z\", \"2021-07-20T07:14:22Z\"),\n",
    "    (\"2021-07-20T07:58:45Z\", \"2021-07-20T07:58:48Z\"),\n",
    "    (\"2021-07-20T09:31:34Z\", \"2021-07-20T09:31:34Z\"),\n",
    "]\n",
    "\n",
    "# Convert to datetime\n",
    "fish_events = [(pd.to_datetime(start), pd.to_datetime(end)) for start, end in fish_events]\n",
    "\n",
    "def label_window(window_start, window_end):\n",
    "    for fish_start, fish_end in fish_events:\n",
    "        # If any overlap, label = 1\n",
    "        if (window_start <= fish_end) and (window_end >= fish_start):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "features_df[\"label\"] = features_df.apply(\n",
    "    lambda row: label_window(row[\"window_start_time\"], row[\"window_end_time\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(features_df[\"label\"].value_counts())\n",
    "display(features_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3f775c-4663-4ac8-8771-9f2253c08093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**RF Model: Trial #1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d22f0601-7a66-4385-bb8a-005efad66032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set up train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature columns \n",
    "feature_cols = [c for c in features_df.columns if c not in [\"label\", \"window_start_time\", \"window_end_time\"]]\n",
    "\n",
    "x = features_df[feature_cols]\n",
    "y = features_df[\"label\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y,\n",
    "    stratify=y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(x_train)}, Test size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846446f1-0221-4ecd-82da-b38add765221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Run the RF model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight=\"balanced\", #This take into account that the 0/1 ratio is off\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630c7056-917b-4389-be89-4b8d337fc328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5e3e0b-d2bf-4a1f-b069-4249c73d371d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Try with downsampling instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c969711-9f3e-48f8-859f-edb6daf5f928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Balance the counts (Downsample)\n",
    "\n",
    "# separate into events vs non events\n",
    "positives = features_df[features_df[\"label\"] == 1]\n",
    "negatives = features_df[features_df[\"label\"] == 0]\n",
    "\n",
    "print(\"Original counts:\")\n",
    "print(\"Positives:\", len(positives))\n",
    "print(\"Negatives:\", len(negatives))\n",
    "\n",
    "# to get about 1:2 ratio:\n",
    "n_negatives_to_keep = len(positives) * 2\n",
    "\n",
    "# Randomly sample negatives (reproducible)\n",
    "negatives_sampled = negatives.sample(n=n_negatives_to_keep, random_state=42)\n",
    "\n",
    "# Combine back into a balanced df \n",
    "balanced_df = pd.concat([positives, negatives_sampled]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Balanced counts:\")\n",
    "print(balanced_df[\"label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222da954-9dda-46a3-b212-1ae0775455c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set up train/test split (Downsample)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define feature columns \n",
    "feature_cols = [c for c in balanced_df.columns if c not in [\"label\", \"window_start_time\", \"window_end_time\"]]\n",
    "\n",
    "x = balanced_df[feature_cols]\n",
    "y = balanced_df[\"label\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y,\n",
    "    stratify=y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(x_train)}, Test size: {len(x_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01547b4-a621-4fd3-896a-505c83b9373e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Run the RF model (DOWNSAMPLED)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2766c26c-ca46-414e-b813-10b0590656c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate DS model\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RFTrial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
