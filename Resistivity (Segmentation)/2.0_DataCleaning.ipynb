{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567ad32d-3ca4-4011-84d9-12099a3957e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Pre-Processing** \n",
    "\n",
    "Loop through the raw daily .csv files in the dbfs and align with the master event file so we have fish events aligned to the resistivity data. Save the labelled data to a new folder called labelled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d534c9-7b46-4e59-88b4-46f6941f8cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start of by aligning the raw resistivity with the fish events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b7082a-73f5-437f-984a-712520ffc6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load and prepare events file with all of the known fish events\n",
    "events_path = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/Thr 20 process/test_KMThu16_2021_07_eventonly.csv\"\n",
    "events = pd.read_csv(events_path, header=0)\n",
    "\n",
    "# Ensure datetime is timezone-aware\n",
    "events['Time'] = pd.to_datetime(events['Time'], utc=True)\n",
    "\n",
    "# Create +/- event windows 2.5 seconds before and 2.5 seconds after as per AF parameters - we might want to truncate this?\n",
    "events['start_time'] = events['Time'] - pd.Timedelta(seconds=2.5)\n",
    "events['end_time']   = events['Time'] + pd.Timedelta(seconds=2.5)\n",
    "\n",
    "\n",
    "# Function to label fish presence from the timestamps in the raw df \n",
    "def label_fish_presence(df, events_df):\n",
    "    df['Time'] = pd.to_datetime(df['Time'], utc=True)\n",
    "    df['fish_present'] = 0\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        mask = (df['Time'] >= event['start_time']) & (df['Time'] <= event['end_time'])\n",
    "        df.loc[mask, 'fish_present'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Loop through all raw daily datasets\n",
    "input_pattern = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/DuplicateFree/*_no_duplicate\"  \n",
    "output_folder = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/labeled\" # where to put the labelled dailies\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run labelling function on all of the daily datasets\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Load and prep daily data\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    df.columns = [\"timestamp\", \"upstream\", \"downstream\"]\n",
    "\n",
    "    # Convert ms timestamp to datetime making sure it is timezone aware too\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "    df = df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "    # Add the differential conductance to the daily df \n",
    "    df[\"differential_conductance\"] = (df[\"downstream\"] - df[\"upstream\"]) / 2\n",
    "\n",
    "    # Label fish events using the function \n",
    "    labeled_df = label_fish_presence(df, events)\n",
    "\n",
    "    # Save to new CSVs in the output folder\n",
    "    filename = os.path.basename(file_path).replace(\"_no_duplicate\", \"_labelled\")\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    labeled_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"All datasets labelled and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84bab9a2-eea0-4c86-a784-dd3a8d9a7ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Look at the df to make sure it is structured as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a1aaf4-7995-4c03-aef1-0fd2a9125e3d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"op\":\"OR\",\"filterGroupId\":\"fg_39281544\",\"filters\":[{\"filterId\":\"f_14c7c2c9\",\"columnId\":\"fish_present\",\"enabled\":true,\"dataType\":\"integer\",\"filterType\":\"eq\",\"filterValue\":\"1\",\"filterValues\":[],\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1756721826731}],\"syncTimestamp\":1756721828694}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"fish_present\"},{\"kind\":\"literal\",\"value\":\"1\",\"type\":\"integer\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks filtered table. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"aW1wb3J0IHBhbmRhcyBhcyBwZAoKZGYgPSBwZC5yZWFkX2NzdigiL2RiZnMvbW50L2xhYi91bnJlc3RyaWN0ZWQvcmFjaGVsLmxlbm5vbkBkZWZyYS5nb3YudWsvbGFiZWxlZC90ZXN0X0tNVGh1MTZfMjAyMV8wN18xN18xNV8zNl8yNlpfbGFiZWxsZWQiKQoKZGlzcGxheShkZik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView2d5e36f\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView2d5e36f\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView2d5e36f\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView2d5e36f) SELECT * FROM q WHERE `fish_present` IN (1)\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView2d5e36f\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Table",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "TABLE"
         },
         {
          "key": "options",
          "value": {}
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "backendAggTable",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1756721855144,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         1243
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "ba085b76-2817-4b88-9ea3-1dde799fa975",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 0.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1756721828955,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "filters": [
          {
           "args": [
            {
             "args": [
              {
               "identifier": "fish_present",
               "kind": "identifier"
              },
              {
               "kind": "literal",
               "type": "integer",
               "value": "1"
              }
             ],
             "function": "in",
             "kind": "call"
            }
           ],
           "function": "or",
           "kind": "call"
          }
         ]
        }
       },
       "submitTime": 1756721828846,
       "subtype": "tableResultSubCmd.backendAggTable",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/labeled/test_KMThu16_2021_07_17_15_36_26Z_labelled\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a0c872-66b7-488c-a77e-e50aa65f2d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check the average sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2227a5f1-5060-4ddd-bf45-0cd4928cb6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to files\n",
    "input_pattern = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/labeled/*Z_labelled\"\n",
    "\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert Time column to datetime\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format='ISO8601')\n",
    "    \n",
    "    # Sort by time\n",
    "    df = df.sort_values('Time')\n",
    "    \n",
    "    # Calculate time differences in seconds\n",
    "    df['delta_t'] = df['Time'].diff().dt.total_seconds()\n",
    "    \n",
    "    # Drop the first NaN\n",
    "    delta_t = df['delta_t'].dropna()\n",
    "    \n",
    "    # Compute average sampling rate\n",
    "    avg_sampling_rate = 1 / delta_t.mean()\n",
    "    \n",
    "    print(f\"Average sampling rate for {file_path}: {avg_sampling_rate:.2f} Hz\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60ec44da-946f-4628-9e20-d1cdbe9df562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Re-Sampling**\n",
    "\n",
    "We have varying sample rate so we need to resample the data. Done using polyphase filtering with automatically derived FIR (Finite Impulse Response) filter to avoid ailising of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6246baaa-1b99-46ac-bbf6-2d4d6ce597f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fractions import Fraction\n",
    "from scipy.signal import resample_poly\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to files\n",
    "input_pattern = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/labeled/*Z_labelled\"\n",
    "output_folder = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/resampled\" # where to put the resampled dailies\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Resampling {file_path}...\")\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format = \"ISO8601\")\n",
    "    df = df.sort_values('Time')\n",
    "    \n",
    "    # Extract signal\n",
    "    signal = df['differential_conductance'].values\n",
    "    \n",
    "    # Original sampling rate\n",
    "    delta_t = df['Time'].diff().dt.total_seconds().dropna()\n",
    "    fs_original = 1 / delta_t.mean()\n",
    "    fs_target = 100\n",
    "    \n",
    "    # Compute up/down ratio\n",
    "    ratio = Fraction(str(fs_target)) / Fraction(str(fs_original))\n",
    "    ratio = ratio.limit_denominator()\n",
    "    up, down = ratio.numerator, ratio.denominator\n",
    "    \n",
    "    # Resample signal\n",
    "    signal_resampled = resample_poly(signal, up, down)\n",
    "    \n",
    "    # Build new uniform time index\n",
    "    n_samples = len(signal_resampled)\n",
    "    start_time = df['Time'].iloc[0]\n",
    "    new_time_index = pd.date_range(start=start_time, periods=n_samples, freq=f\"{1000/fs_target}ms\")\n",
    "    \n",
    "    # Convert times to seconds since start for binning\n",
    "    original_times_sec = (df['Time'] - start_time).dt.total_seconds()\n",
    "    bin_edges_sec = np.append(np.arange(n_samples) / fs_target, n_samples / fs_target)\n",
    "    \n",
    "    # Get bin index for each original row\n",
    "    bin_indices = np.digitize(original_times_sec, bin_edges_sec) - 1\n",
    "    \n",
    "    # Vectorized: group fish_present by bin index and take max (1 if any fish present)\n",
    "    labels_resampled = (\n",
    "    pd.Series(df['fish_present'].values)\n",
    "    .groupby(bin_indices)\n",
    "    .max()\n",
    "    .reindex(range(n_samples), fill_value=0)\n",
    "    .astype(int)\n",
    "    .values\n",
    "    )\n",
    "    \n",
    "    # Build resampled DataFrame\n",
    "    df_resampled = pd.DataFrame({\n",
    "    'Time': new_time_index,\n",
    "    'differential_conductance': signal_resampled,\n",
    "    'fish_present': labels_resampled\n",
    "    })\n",
    "\n",
    "        # Save to new CSVs in the output folder\n",
    "    filename = os.path.basename(file_path).replace(\"_labelled\", \"_resampled\")\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    df_resampled.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"All datasets resampled and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b85ba9dc-2033-49f5-89c8-e67c8f39e8ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Bin into 5 second chunks**\n",
    "\n",
    "\n",
    "We need to have windows of time to feed into the UNet, do this e = 1/5 Hz as an initial approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cba1b0c6-7c9c-4281-810f-32a5fd3c8c87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Parameters\n",
    "input_folder = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/resampled/*Z_resampled\"  # folder with CSVs\n",
    "output_folder = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/cleaned/*Z_cleaned\"\n",
    "window_size = 600                  # 6 seconds per window\n",
    "step_size = 500                    # 1 second stride \n",
    "\n",
    "# Make sure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to scale a window\n",
    "def scale_window(chunk):\n",
    "    median = np.median(chunk)\n",
    "    iqr = np.percentile(chunk, 75) - np.percentile(chunk, 25)\n",
    "    if iqr == 0:\n",
    "        return chunk - median  # avoid division by zero\n",
    "    return (chunk - median) / iqr\n",
    "\n",
    "# Process each CSV\n",
    "all_windows = []\n",
    "\n",
    "for csv_file in glob(os.path.join(input_folder)):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    signal = df['differential_conductance'].to_numpy()\n",
    "    fish_present = df['fish_present'].to_numpy()\n",
    "    \n",
    "    # Bin into windows\n",
    "    for start in range(0, len(signal) - window_size + 1, step_size):\n",
    "        end = start + window_size\n",
    "        window = signal[start:end]\n",
    "        label = 1 if fish_present[start:end].sum() > 0.5 else 0\n",
    "        \n",
    "        # Scale\n",
    "        window_scaled = scale_window(window)\n",
    "        \n",
    "        # Append as dictionary\n",
    "        all_windows.append({\n",
    "            \"signal_scaled\": window_scaled.tolist(),\n",
    "            \"fish_present\": label\n",
    "        })\n",
    "\n",
    "\n",
    "# Combine all windows into a single DataFrame\n",
    "fish_clean = pd.DataFrame(all_windows)\n",
    "\n",
    "# Save to cleaned folder\n",
    "fish_clean.to_csv(os.path.join(output_folder, \"fish_clean.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7865542-2f29-4336-b548-c40476bd8e47",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756472779947}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_d4cf9198\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_c73c450e\",\"enabled\":true,\"columnId\":\"fish_present\",\"dataType\":\"integer\",\"filterType\":\"eq\",\"filterValue\":\"1\",\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1756472787318}],\"syncTimestamp\":1756472787319}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"fish_present\"},{\"kind\":\"literal\",\"value\":\"1\",\"type\":\"integer\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks filtered table. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmaXNoX2NsZWFuKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView4a62a16\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView4a62a16\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView4a62a16\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView4a62a16) SELECT * FROM q WHERE `fish_present` IN (1)\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView4a62a16\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Table",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "TABLE"
         },
         {
          "key": "options",
          "value": {}
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "backendAggTable",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1756737964818,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         172
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "03a82473-5a25-4a33-a5d3-a6f0ab1cd5fe",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 0.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1756737946832,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "filters": [
          {
           "args": [
            {
             "args": [
              {
               "identifier": "fish_present",
               "kind": "identifier"
              },
              {
               "kind": "literal",
               "type": "integer",
               "value": "1"
              }
             ],
             "function": "in",
             "kind": "call"
            }
           ],
           "function": "or",
           "kind": "call"
          }
         ]
        }
       },
       "submitTime": 1756737927272,
       "subtype": "tableResultSubCmd.backendAggTable",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(fish_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "682dd378-4387-48f4-bb8a-ceb20e17f90c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Balance training dataset**\n",
    "\n",
    "The noise (0/no fish) to signal (1/fish) ratio is out of whack. We need to balance it so the ratio is more 1:1 and can be used as a training dataset. Otherwise it could label everything as 0 and we would still get 99% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97c44f9-bf6e-4784-92b3-d6d17d7c50d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get the paths to the right folders\n",
    "input_file = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/cleaned/*Z_cleaned/fish_clean.csv\" # Where the labelled daily datasets are\n",
    "output_folder = \"/dbfs/mnt/lab/unrestricted/rachel.lennon@defra.gov.uk/cleaned/\" # Where to put the balanced df \n",
    "\n",
    "# Run through all of the labelled daily df and balance the noise to signal\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "    \n",
    "# Split fish vs no-fish\n",
    "fish_df = df[df['fish_present'] == 1]\n",
    "no_fish_df = df[df['fish_present'] == 0]\n",
    "    \n",
    "# Match counts so equalised\n",
    "no_fish_sample = no_fish_df.sample(len(fish_df), random_state=42)\n",
    "    \n",
    "# Combine and shuffle\n",
    "balanced_df = pd.concat([fish_df, no_fish_sample]).sample(frac=1, random_state=42)\n",
    "\n",
    "# Save balanced CSV\n",
    "balanced_df.to_csv(os.path.join(output_folder, \"all_balanced.csv\"), index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.0_DataCleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
