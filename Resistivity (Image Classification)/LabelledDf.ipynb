{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89e62505-546b-4027-86d9-ba5fdf194a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Generate labelled resistivity data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "310ea0ec-4a43-4f1f-848e-17fbb6938515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Loop through the raw daily .csv files in the dbfs and align with the master event file (too big to combine and do in one). Save the labelled data to a new folder called labelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e5429b-0da1-4034-9cb0-3a9a235406ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load and prepare events file\n",
    "\n",
    "events_path = \"/dbfs/FileStore/rachlenn/Thr 20 process/test_KMThu16_2021_07.csv\"\n",
    "events = pd.read_csv(events_path, header=0)\n",
    "\n",
    "# Ensure datetime is timezone-aware\n",
    "events['Time'] = pd.to_datetime(events['Time'], utc=True)\n",
    "\n",
    "# Create +/- event windows\n",
    "events['start_time'] = events['Time'] - pd.Timedelta(seconds=2)\n",
    "events['end_time']   = events['Time'] + pd.Timedelta(seconds=8)\n",
    "\n",
    "\n",
    "# Function to label fish presence\n",
    "def label_fish_presence(df, events_df):\n",
    "    df['Time'] = pd.to_datetime(df['Time'], utc=True)\n",
    "    df['fish_present'] = 0\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        mask = (df['Time'] >= event['start_time']) & (df['Time'] <= event['end_time'])\n",
    "        df.loc[mask, 'fish_present'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Loop through all daily datasets\n",
    "input_pattern = \"/dbfs/FileStore/rachlenn/Thr 20 process/*Z\"  # match your naming\n",
    "output_folder = \"/dbfs/FileStore/rachlenn/labeled\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Load and prep daily data\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    df.columns = [\"timestamp\", \"upstream\", \"downstream\"]\n",
    "\n",
    "    # Convert ms timestamp to datetime\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "    df = df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "    # Add extra features\n",
    "    df[\"differential_conductance\"] = (df[\"downstream\"] - df[\"upstream\"]) / 2\n",
    "\n",
    "    # Label fish events\n",
    "    labeled_df = label_fish_presence(df, events)\n",
    "\n",
    "    # Save to new CSV\n",
    "    filename = os.path.basename(file_path).replace(\".csv\", \"_labeled.csv\")\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    labeled_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"✅ All datasets processed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1083af38-4f78-4c98-9e8b-15d664b41b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path where your labeled CSVs are stored\n",
    "input_folder = \"/dbfs/FileStore/rachlenn/labeled/\"\n",
    "output_folder = \"/dbfs/FileStore/rachlenn/balanced/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all CSV files in input folder\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"*Z\"))\n",
    "\n",
    "balanced_dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Split fish vs no-fish\n",
    "    fish_df = df[df['fish_present'] == 1]\n",
    "    no_fish_df = df[df['fish_present'] == 0]\n",
    "    \n",
    "    # Match counts\n",
    "    no_fish_sample = no_fish_df.sample(len(fish_df), random_state=42)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat([fish_df, no_fish_sample]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Save balanced CSV\n",
    "    filename = os.path.basename(file).replace(\".csv\", \"_balanced.csv\")\n",
    "    balanced_path = os.path.join(output_folder, filename)\n",
    "    balanced_df.to_csv(balanced_path, index=False)\n",
    "    \n",
    "    balanced_dfs.append(balanced_df)\n",
    "\n",
    "# Optional: Merge into one big dataset\n",
    "merged_df = pd.concat(balanced_dfs).sample(frac=1, random_state=42)\n",
    "merged_df.to_csv(os.path.join(output_folder, \"all_balanced.csv\"), index=False)\n",
    "\n",
    "print(\"✅ Done — balanced CSVs saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67fff52-7d85-4ebd-8f75-15def178d618",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754667818442}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(merged_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LabelledDf",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
