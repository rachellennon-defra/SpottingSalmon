{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5e6d5b-e0b9-4598-8a6c-2b2091836a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Data Cleaning with Spark**\n",
    "\n",
    "We have quite a lot of big dataframes, so lets also try the same as 2.0 but written in spark and compare run times. \n",
    "\n",
    "**Generate labelled resistivity data**\n",
    "\n",
    "\n",
    "Start of by aligning the raw resistivity with the fish events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7d6c4bd-c5ec-4f53-8491-002e79c1d2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load and prepare events file with all of the known fish events\n",
    "events_path = \"/FileStore/rachlenn/Thr 20 process/test_KMThu16_2021_07_eventonly.csv\"\n",
    "\n",
    "# Name the columns\n",
    "events_df = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .csv(events_path)\n",
    "        .withColumn(\"Time\", F.to_timestamp(\"Time\", \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\")) # make it time \n",
    "        .withColumn(\"Time\", F.from_utc_timestamp(\"Time\", \"UTC\"))  # ensure UTC aware\n",
    "        .withColumn(\"start_time\", F.col(\"Time\") - F.expr(\"INTERVAL 2.5 SECONDS\")) # Create +/- event windows 2 seconds before\n",
    "        .withColumn(\"end_time\",   F.col(\"Time\") + F.expr(\"INTERVAL 2.5 SECONDS\")) # And 2 seconds after\n",
    ")\n",
    "\n",
    "# Function to label fish presence\n",
    "def label_fish_presence(df, events_df):\n",
    "    # Join condition: df.Time between event.start_time and event.end_time\n",
    "    joined = (\n",
    "        df.join(\n",
    "            events_df.select(\"start_time\", \"end_time\"),\n",
    "            (df.Time >= events_df.start_time) & (df.Time <= events_df.end_time),\n",
    "            \"left\"\n",
    "        )\n",
    "        .withColumn(\"fish_present\", F.when(F.col(\"start_time\").isNotNull(), F.lit(1)).otherwise(F.lit(0)))\n",
    "        .drop(\"start_time\", \"end_time\")\n",
    "    )\n",
    "    return joined\n",
    "\n",
    "#  Process each daily dataset to align with fish event\n",
    "input_pattern = \"/FileStore/rachlenn/DuplicateFree/*_no_duplicate\"\n",
    "output_folder = \"/FileStore/rachlenn/labeled\"\n",
    "\n",
    "# Get list of files using dbutils\n",
    "file_paths = [f.path for f in dbutils.fs.ls(input_pattern.replace(\"*_no_duplicate\", \"\")) if f.path.endswith(\"_no_duplicate\")]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Load daily data\n",
    "    df = (\n",
    "        spark.read\n",
    "            .option(\"header\", True)\n",
    "            .csv(file_path)\n",
    "            .toDF(\"timestamp\", \"upstream\", \"downstream\")\n",
    "            .withColumn(\"Time\", F.from_utc_timestamp((F.col(\"timestamp\")/1000).cast(T.TimestampType()), \"UTC\"))\n",
    "            .drop(\"timestamp\")\n",
    "            .withColumn(\"upstream\",   F.col(\"upstream\").cast(\"double\"))\n",
    "            .withColumn(\"downstream\", F.col(\"downstream\").cast(\"double\"))\n",
    "            .withColumn(\"differential_conductance\", (F.col(\"downstream\") - F.col(\"upstream\")) / 2)\n",
    "    )\n",
    "\n",
    "    # Label fish events\n",
    "    labeled_df = label_fish_presence(df, events_df)\n",
    "\n",
    "    # Save\n",
    "    filename = os.path.basename(file_path).replace(\"_no_duplicate\", \"_S_labelled\")\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    labeled_df.write.csv(save_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"All datasets labelled and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1b59660-cda0-45bd-a2c9-787319475e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lets take a wee look at what one of the df looks like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f0229e-1133-4f1c-a978-6deccf68c5e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"dbfs:/FileStore/rachlenn/labeled/test_KMThu16_2021_07_17_15_36_26Z_S_labelled\")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11f47f46-73f5-4c74-a1e4-865d0840bd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Determine what the average sampling rate is for window bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d4c2a7-89a6-4dff-b3e8-b684f48f6a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate sps \n",
    "\n",
    "#Calculate the sampling rate\n",
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Read all CSVs into one Spark DataFrame\n",
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)  # set False if files have no header\n",
    "         .csv(\"dbfs:/FileStore/rachlenn/labeled/*_S_labelled\")\n",
    "         .withColumn(\"Time\", F.to_timestamp(\"Time\"))\n",
    ")\n",
    "\n",
    "# Sort by Time\n",
    "w = Window.orderBy(\"Time\")\n",
    "\n",
    "# Calculate delta in seconds between consecutive rows\n",
    "df_with_delta = df.withColumn(\n",
    "    \"delta_s\",\n",
    "    (F.col(\"Time\").cast(\"long\") - F.lag(\"Time\").over(w).cast(\"long\")).cast(\"double\")\n",
    ")\n",
    "\n",
    "# Compute average sampling rate\n",
    "avg_rate_df = (\n",
    "    df_with_delta.filter(F.col(\"delta_s\").isNotNull())\n",
    "                 .agg((F.lit(1) / F.avg(\"delta_s\")).alias(\"avg_hz\"))\n",
    ")\n",
    "\n",
    "avg_rate = avg_rate_df.collect()[0][\"avg_hz\"]\n",
    "print(f\"Average sampling rate: {avg_rate:.2f} Hz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65c512ed-e623-401e-b442-6e89aac5289f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Extract Windows**\n",
    "\n",
    "The ML models should be fed windows of the event, so need to generate these before balancing the date stream. \n",
    "\n",
    "We need the windows to be of fixed n arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb0563c8-f4cb-49e0-9599-1c5f9702d9da",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755098083282}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1755098293728}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Load labeled data (with headers)\n",
    "df = spark.read.option(\"header\", True).csv(\n",
    "    \"dbfs:/FileStore/rachlenn/labeled/test_KMThu16_2021_07_17_15_36_26Z_labelled\"\n",
    ")\n",
    "\n",
    "# Ensure proper types\n",
    "df = (df\n",
    "      .withColumn(\"Time\", F.to_timestamp(\"Time\"))\n",
    "      .withColumn(\"upstream\", F.col(\"upstream\").cast(\"double\"))\n",
    "      .withColumn(\"downstream\", F.col(\"downstream\").cast(\"double\"))\n",
    "      .withColumn(\"differential_conductance\", F.col(\"differential_conductance\").cast(\"double\"))\n",
    "      .withColumn(\"fish_present\", F.col(\"fish_present\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Create 10-second window start timestamps\n",
    "df = df.withColumn(\"window_start\",\n",
    "                   (F.unix_timestamp(\"Time\") / 10).cast(\"long\") * 10)\n",
    "\n",
    "# Aggregate to window level\n",
    "window_df = (df.groupBy(\"window_start\")\n",
    "               .agg(\n",
    "                   F.collect_list(\"upstream\").alias(\"upstream_series\"),\n",
    "                   F.collect_list(\"downstream\").alias(\"downstream_series\"),\n",
    "                   F.collect_list(\"differential_conductance\").alias(\"diff_series\"),\n",
    "                   F.max(\"fish_present\").alias(\"fish_present\")  # label = 1 if any row has fish\n",
    "               )\n",
    "               .withColumn(\"window_start_time\", F.from_unixtime(\"window_start\"))\n",
    "               .drop(\"window_start\")\n",
    "            )\n",
    "\n",
    "display(window_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.1_DataCleaningSPARK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
