{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567ad32d-3ca4-4011-84d9-12099a3957e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Pre-Processing** \n",
    "\n",
    "Loop through the raw daily .csv files in the dbfs and align with the master event file so we have fish events aligned to the resistivity data. Save the labelled data to a new folder called labelled. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d534c9-7b46-4e59-88b4-46f6941f8cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start of by aligning the raw resistivity with the fish events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49b7082a-73f5-437f-984a-712520ffc6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Load and prepare events file with all of the known fish events\n",
    "events_path = \"/dbfs/FileStore/rachlenn/Thr 20 process/test_KMThu16_2021_07_eventonly.csv\"\n",
    "events = pd.read_csv(events_path, header=0)\n",
    "\n",
    "# Ensure datetime is timezone-aware\n",
    "events['Time'] = pd.to_datetime(events['Time'], utc=True)\n",
    "\n",
    "# Create +/- event windows 2 seconds before and 2 seconds after as per AF parameters - we might want to truncate this?\n",
    "events['start_time'] = events['Time'] - pd.Timedelta(seconds=2.5)\n",
    "events['end_time']   = events['Time'] + pd.Timedelta(seconds=2.5)\n",
    "\n",
    "\n",
    "# Function to label fish presence from the timestamps in the raw df \n",
    "def label_fish_presence(df, events_df):\n",
    "    df['Time'] = pd.to_datetime(df['Time'], utc=True)\n",
    "    df['fish_present'] = 0\n",
    "    \n",
    "    for _, event in events_df.iterrows():\n",
    "        mask = (df['Time'] >= event['start_time']) & (df['Time'] <= event['end_time'])\n",
    "        df.loc[mask, 'fish_present'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Loop through all raw daily datasets\n",
    "input_pattern = \"/dbfs/FileStore/rachlenn/DuplicateFree/*_no_duplicate\"  \n",
    "output_folder = \"/dbfs/FileStore/rachlenn/labeled\" # where to put the labelled dailies\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Run labelling function on all of the daily datasets\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "\n",
    "    # Load and prep daily data\n",
    "    df = pd.read_csv(file_path, header=0)\n",
    "    df.columns = [\"timestamp\", \"upstream\", \"downstream\"]\n",
    "\n",
    "    # Convert ms timestamp to datetime making sure it is timezone aware too\n",
    "    df[\"Time\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\", utc=True)\n",
    "    df = df.drop(columns=[\"timestamp\"])\n",
    "\n",
    "    # Add the differential conductance to the daily df \n",
    "    df[\"differential_conductance\"] = (df[\"downstream\"] - df[\"upstream\"]) / 2\n",
    "\n",
    "    # Label fish events using the function \n",
    "    labeled_df = label_fish_presence(df, events)\n",
    "\n",
    "    # Save to new CSVs in the output folder\n",
    "    filename = os.path.basename(file_path).replace(\"_no_duplicate\", \"_labelled\")\n",
    "    save_path = os.path.join(output_folder, filename)\n",
    "    labeled_df.to_csv(save_path, index=False)\n",
    "\n",
    "print(\"All datasets labelled and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84bab9a2-eea0-4c86-a784-dd3a8d9a7ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Look at the df to make sure it is structured as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a1aaf4-7995-4c03-aef1-0fd2a9125e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/dbfs/FileStore/rachlenn/labeled/test_KMThu16_2021_07_17_15_36_26Z_labelled\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a0c872-66b7-488c-a77e-e50aa65f2d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Check the average sampling rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2227a5f1-5060-4ddd-bf45-0cd4928cb6e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Path to files\n",
    "input_pattern = \"/dbfs/FileStore/rachlenn/labeled/*Z_labelled\"\n",
    "\n",
    "for file_path in glob.glob(input_pattern):\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert Time column to datetime\n",
    "    df['Time'] = pd.to_datetime(df['Time'], format='ISO8601')\n",
    "    \n",
    "    # Sort by time\n",
    "    df = df.sort_values('Time')\n",
    "    \n",
    "    # Calculate time differences in seconds\n",
    "    df['delta_t'] = df['Time'].diff().dt.total_seconds()\n",
    "    \n",
    "    # Drop the first NaN\n",
    "    delta_t = df['delta_t'].dropna()\n",
    "    \n",
    "    # Compute average sampling rate\n",
    "    avg_sampling_rate = 1 / delta_t.mean()\n",
    "    \n",
    "    print(f\"Average sampling rate for {file_path}: {avg_sampling_rate:.2f} Hz\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60ec44da-946f-4628-9e20-d1cdbe9df562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Re-Sampling**\n",
    "\n",
    "We have varying sample rate so we need to resample the data. Done using polyphase filtering with automatically derived FIR (Finite Impulse Response) filter to avoid ailising of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6246baaa-1b99-46ac-bbf6-2d4d6ce597f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fractions import Fraction\n",
    "from scipy.signal import resample_poly\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/dbfs/FileStore/rachlenn/labeled/test_KMThu16_2021_07_17_15_36_26Z_labelled\")\n",
    "df['Time'] = pd.to_datetime(df['Time'], format = \"ISO8601\")\n",
    "df = df.sort_values('Time')\n",
    "\n",
    "# Extract signal\n",
    "signal = df['differential_conductance'].values\n",
    "\n",
    "# Original sampling rate\n",
    "delta_t = df['Time'].diff().dt.total_seconds().dropna()\n",
    "fs_original = 1 / delta_t.mean()\n",
    "fs_target = 100\n",
    "\n",
    "# Compute up/down ratio\n",
    "ratio = Fraction(str(fs_target)) / Fraction(str(fs_original))\n",
    "ratio = ratio.limit_denominator()\n",
    "up, down = ratio.numerator, ratio.denominator\n",
    "\n",
    "# Resample signal\n",
    "signal_resampled = resample_poly(signal, up, down)\n",
    "\n",
    "# Build new uniform time index\n",
    "n_samples = len(signal_resampled)\n",
    "start_time = df['Time'].iloc[0]\n",
    "new_time_index = pd.date_range(start=start_time, periods=n_samples, freq=f\"{1000/fs_target}ms\")\n",
    "\n",
    "# Convert times to seconds since start for binning\n",
    "original_times_sec = (df['Time'] - start_time).dt.total_seconds()\n",
    "bin_edges_sec = np.append(np.arange(n_samples) / fs_target, n_samples / fs_target)\n",
    "\n",
    "# Get bin index for each original row\n",
    "bin_indices = np.digitize(original_times_sec, bin_edges_sec) - 1\n",
    "\n",
    "# Vectorized: group fish_present by bin index and take max (1 if any fish present)\n",
    "labels_resampled = (\n",
    "    pd.Series(df['fish_present'].values)\n",
    "    .groupby(bin_indices)\n",
    "    .max()\n",
    "    .reindex(range(n_samples), fill_value=0)\n",
    "    .astype(int)\n",
    "    .values\n",
    ")\n",
    "\n",
    "# Build resampled DataFrame\n",
    "df_resampled = pd.DataFrame({\n",
    "    'Time': new_time_index,\n",
    "    'differential_conductance': signal_resampled,\n",
    "    'fish_present': labels_resampled\n",
    "})\n",
    "\n",
    "print(df_resampled.head(20))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b85ba9dc-2033-49f5-89c8-e67c8f39e8ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Bin into 10 second chunks**\n",
    "\n",
    "\n",
    "We need to have windows of time to feed into the UNet, do this e = 1/10 Hz as an initial approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a0b9390-3a4c-4660-b348-f051d8c2f85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "window_size_seconds = 10\n",
    "samples_per_window = fs_target * window_size_seconds  # e.g., 100 Hz * 10 s = 1000 samples\n",
    "\n",
    "# Drop incomplete final chunk\n",
    "n_complete_chunks = len(df_resampled) // samples_per_window\n",
    "\n",
    "# Reshape signal\n",
    "signal_chunks = signal_resampled[:n_complete_chunks * samples_per_window].reshape(\n",
    "    n_complete_chunks, samples_per_window\n",
    ")\n",
    "\n",
    "# Reshape labels (binary, so we take max across window later)\n",
    "label_chunks = labels_resampled[:n_complete_chunks * samples_per_window].reshape(\n",
    "    n_complete_chunks, samples_per_window\n",
    ")\n",
    "\n",
    "# Window-level labels: 1 if fish present anywhere in window\n",
    "window_labels = (label_chunks.max(axis=1)).astype(int)\n",
    "\n",
    "# Build a DataFrame where each row is a 10-second chunk\n",
    "df_chunks = pd.DataFrame({\n",
    "    'signal': list(signal_chunks),   # each row is an array of 1000 samples\n",
    "    'fish_present': window_labels\n",
    "})\n",
    "\n",
    "print(df_chunks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ab8958-cb18-40bc-aa67-0a0ff145d0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Scale**\n",
    "\n",
    "Individually scale each window to median 0 and interquartile range to provide a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c8f270-e442-40b8-b7c8-2b4daddedb0b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755183095748}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_1eefc67f\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_dbbb7b13\",\"enabled\":true,\"columnId\":\"fish_present\",\"dataType\":\"integer\",\"filterType\":\"eq\",\"filterValue\":\"1\",\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1755183196932}],\"syncTimestamp\":1755183196933}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"fish_present\"},{\"kind\":\"literal\",\"value\":\"1\",\"type\":\"integer\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks filtered table. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2NhbGVfY2h1bmsoY2h1bmspOgogICAgIiIiU2NhbGUgYSAxRCBhcnJheSB0byBtZWRpYW4gMCwgSVFSIDEiIiIKICAgIG1lZGlhbiA9IG5wLm1lZGlhbihjaHVuaykKICAgIGlxciA9IG5wLnBlcmNlbnRpbGUoY2h1bmssIDc1KSAtIG5wLnBlcmNlbnRpbGUoY2h1bmssIDI1KQogICAgaWYgaXFyID09IDA6CiAgICAgICAgcmV0dXJuIGNodW5rIC0gbWVkaWFuICAjIGF2b2lkIGRpdmlzaW9uIGJ5IHplcm8KICAgIHJldHVybiAoY2h1bmsgLSBtZWRpYW4pIC8gaXFyCgojIEFwcGx5IHNjYWxpbmcgdG8gZWFjaCBjaHVuawpkZl9jaHVua3NbJ3NpZ25hbF9zY2FsZWQnXSA9IGRmX2NodW5rc1snc2lnbmFsJ10uYXBwbHkoc2NhbGVfY2h1bmspCgpkaXNwbGF5KGRmX2NodW5rcykK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewceec1a4\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewceec1a4\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewceec1a4\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewceec1a4) SELECT * FROM q WHERE `fish_present` IN (1)\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewceec1a4\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Table",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "TABLE"
         },
         {
          "key": "options",
          "value": {}
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "backendAggTable",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1755183207514,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         3
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "19f74709-951a-4e09-886c-075035b46c7e",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 0.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1755183200534,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "filters": [
          {
           "args": [
            {
             "args": [
              {
               "identifier": "fish_present",
               "kind": "identifier"
              },
              {
               "kind": "literal",
               "type": "integer",
               "value": "1"
              }
             ],
             "function": "in",
             "kind": "call"
            }
           ],
           "function": "or",
           "kind": "call"
          }
         ]
        }
       },
       "submitTime": 1755183200461,
       "subtype": "tableResultSubCmd.backendAggTable",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def scale_chunk(chunk):\n",
    "    \"\"\"Scale a 1D array to median 0, IQR 1\"\"\"\n",
    "    median = np.median(chunk)\n",
    "    iqr = np.percentile(chunk, 75) - np.percentile(chunk, 25)\n",
    "    if iqr == 0:\n",
    "        return chunk - median  # avoid division by zero\n",
    "    return (chunk - median) / iqr\n",
    "\n",
    "# Apply scaling to each chunk\n",
    "df_chunks['signal_scaled'] = df_chunks['signal'].apply(scale_chunk)\n",
    "\n",
    "display(df_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "682dd378-4387-48f4-bb8a-ceb20e17f90c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Balance training dataset**\n",
    "\n",
    "The noise (0/no fish) to signal (1/fish) ratio is out of whack. We need to balance it so the ratio is more 1:1 and can be used as a training dataset. Otherwise it could label everything as 0 and we would still get 99% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e97c44f9-bf6e-4784-92b3-d6d17d7c50d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get packages\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Get the paths to the right folders\n",
    "input_folder = \"/dbfs/FileStore/rachlenn/labeled/\" # Where the labelled daily datasets are\n",
    "output_folder = \"/dbfs/FileStore/rachlenn/balanced/\" # Where to put the balanced df \n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get all CSV files in input folder that end in Z\n",
    "csv_files = glob.glob(os.path.join(input_folder, \"*Z\"))\n",
    "\n",
    "# Initiate empty df\n",
    "balanced_dfs = []\n",
    "\n",
    "# Run through all of the labelled daily df and balance the noise to signal\n",
    "for file in csv_files:\n",
    "    print(f\"Processing {file}...\")\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Split fish vs no-fish\n",
    "    fish_df = df[df['fish_present'] == 1]\n",
    "    no_fish_df = df[df['fish_present'] == 0]\n",
    "    \n",
    "    # Match counts so equalised\n",
    "    no_fish_sample = no_fish_df.sample(len(fish_df), random_state=42)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    balanced_df = pd.concat([fish_df, no_fish_sample]).sample(frac=1, random_state=42)\n",
    "    \n",
    "    # Save balanced CSV\n",
    "    filename = os.path.basename(file).replace(\".csv\", \"_balanced.csv\")\n",
    "    balanced_path = os.path.join(output_folder, filename)\n",
    "    balanced_df.to_csv(balanced_path, index=False)\n",
    "    \n",
    "    balanced_dfs.append(balanced_df)\n",
    "\n",
    "# Merge into one big dataset\n",
    "merged_df = pd.concat(balanced_dfs).sample(frac=1, random_state=42)\n",
    "merged_df.to_csv(os.path.join(output_folder, \"all_balanced.csv\"), index=False)\n",
    "\n",
    "print(\" All balanced CSVs and master df saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.0_DataCleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
